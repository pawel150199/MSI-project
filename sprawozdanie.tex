% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%

\documentclass{article}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage[polish]{babel}
\usepackage[utf8]{inputenc}
\pagenumbering{arabic}

\begin{document}

\title{Metody Sztucznej Inteligencji - Projekt.}

%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here

\author{Paweł Polski, Michał Włosek, Dariusz Szymula}


\maketitle      
\begin{abstract}
\begin{center}
    Przetwarzanie wstępne w dużych zbiorach danych.
\end{center}
\end{abstract}



%
\section{Opis problemu.}
Pozyskiwanie wiedzy z rzeczywistych zbiorów danych w dzisiejszych czasach jest trudne, ponieważ dane te są często wielowymiarowe, wieloklasowe oraz niezrównoważone. W uczeniu nadzorowanym w celu poprawnego nauczenia modelu lub klasyfikatora potrzebujemy danych, które ten proces umożliwią. Aby przystosować obecnie gromadzone zbiory wprowadzono metody, które umożliwiają ich zrównoważenie. Powszechnie stosowane są techniki próbkowania danych, czyli podpróbkowania (undersampling) lub nadmiernego próbkowania(oversampling). Powodują one zmniejszenie instancji klas w podejściu podpróbkowania oraz powstawania klas mniejszościowych w nadmiernym próbkowaniu. Każda z tych technik wnosi pewne zmiany do wcześniej posiadanych już zbiorów. Różnica pomiędzy undersamplingiem a oversamplingiem została przedstawiona na rysunku nr 1 i nr 2.



\begin{figure}[h!]
\includegraphics[width=\textwidth]{under_oversampling}
\caption{Różnica pomiędzy undersamplingiem a oversamplingiem} 
\label{fig:under_oversampling}
\end{figure}
\break 
\begin{figure}[h!]
\includegraphics[width=\textwidth]{xdd.png}
\caption{Różnica pomiędzy undersamplingiem a oversamplingiem} 
\label{fig:underoversampling}
\end{figure}



W zbiorach dużych danych dąży się do zmniejszenia danych bez utraty danych informacyjnych. Pozwala to zmniejszyć wymagania odnośnie systemów, które przetwarzają te dane. Z metod przetwarzania wstępnego te założenia spełnia undersampling i to na nim się skupimy w naszej pracy. Proces przetwarzania danych z zaznaczonym miejscem, w którym odbywa się przetwarzanie wstępne pokazano na rysunku nr 3.\newline
\break
\begin{figure}[h!]
\includegraphics[width=\textwidth]{wstepne.png}
\caption{Proces przetwarzania wstępnego.} 
\label{fig:under_oversampling}
\end{figure}
\break
\break
\break


\subsection{Duże zbiory danych (Big Data).}
\subsubsection{Big Data}
\hfill \break

Terminem tym określa się zbiory danych tak duże, przy których metody analizy tradycyjnej nie zwracają oczekiwanych wyników bądź zastosowanie ich jest wręcz niemożliwe. Big Data to również dane, które nie mogą być obsługiwane i przetwarzane przez większość obecnych systemów lub metod informatycznych, ponieważ oprócz swojego rozmiaru uniemożliwiające załadowanie ich na pojedyńczą instancję urządzenia, to również wiele tradycyjnych metod analizy danych stowrzonych dla scentralizowanego procesu przetwarzanua ich mogą być niemożliwe w użyciu. Dane wieloskalowe wykorzystuje się zarówno w środowisku komercyjnym, ale również w niekomercyjnym. Na przykładzie firm komercyjnych analiza dużych zbiorów danych może posłużyć do zwiększenia wiedzy na temat personelu, procesów produkcyjnych, produktów oraz klientów. Z drugiej strony organizacje rządowe wykorzystują analizę Big Data m.in w celu wykrywania oszustw, zwiększenia płynności finansowych oraz bezpieczeństwa. Różne organizacje, które mają dostęp do dużych zbiorów danych na konkretny temat mogą wykorzystać wyniki ich analizy w celu opracowania strategii rozwoju na przyszłość. Istnieje definicja zwana również 3V, która wyjaśnia czym są Big Data: objętość, szybkość, różnorodność. Definicja 3V oznacza, że rozmiar danych jest duży, dane będą tworzone szybko, a dane będą istniały odpowiednio w wielu typach i pochodzą z różnych źródeł. Później wykazano, że te pojęcia nie są wystarczające aby wyjaśnić czym są Big Data, dlatego dodano do nich prawdziwość, trafność, wartość, zmienność, miejsce, słownictwo i niejasność, aby uzupełnić wyjaśnienie dużych zbiorów danych. %(Thus, veracity, validity, value, variability, venue, vocabulary, and vagueness were added to make some complement explanation of big data)%


Głównymi kategorami danych są m.in: 
\begin{itemize}
    \item \textbf{strukturalne} - to dane, które zależą od modelu danych i znajdują się w stałym polu w rekordzie,
    \item \textbf{niestrukturalne} - to dane, które nie są łatwe do dopasowania do modelu danych, ponieważ zawartość jest zależne od kontekstu lub zmienne,
    \item \textbf{język naturalny} -  szczególny rodzaj danych nieustrukturyzowanych wymagający wiedzy zarówno na temat danych jak i ligwistyki,
    \item \textbf{dane generowane maszynowo} - to informacje, które są automatycznie tworzone przez komputer, proces, aplikacja lub inna maszyna bez interwencji człowieka,
    \item \textbf{oparte na grafach} - dane wskazują na matematyczną teorię grafów, czyli matematyczną strukturę do modelowania relacji między obiektami,
    \item \textbf{filmy, obrazy, dźwięk} - trudne w analizie dane, ponieważ łatwe w analizie dla człowieka rozpoznawanie obiektów stanowi trudne zadanie dla maszyny,
    \item \textbf{strumieniowane} - dane, które strumieniowo wpływają do systemu
\end{itemize}
Z uwagi, że tworzenie danych jest dużo łatwiejsze niż znajdowanie w nich przydatnych rzeczy wystąpiły problemy z analizowaniem danych wielkoskalowych. \begin{itemize}
    \item \textbf{Nieskalowalne i scentralizowane} - większość metod analizy danych nie jest przeznaczona do działania na dużych i złożonych zbiorach danych. Z tego powodu metody te nie mają atrybutu skalowalności. A przede wszystkim projektując je zakładano, że wszystkie dane znajdą się w pamięci maszyny.
    \item \textbf{Niedynamiczne} - większość tradycyjnych metod nie jest przystosowana do dynamicznej analizy danych wejściowych, dostosowywania się do różnych sytuacji.
    \item \textbf{O jednolitej strukturze danych} - większość problemów z analizą danych, zakłada, że format danych wejściowych będzie taki sam. W Big Data pojawia się problem różnorodności danych wejściowych czyli ich niezbalansowania.
\end{itemize}

W celu rozwiązania tego problemu pojawiły się metody takie jak próbkowanie (podział metod próbkowania opisany jest w punkcie \ref{sampling_methods}), kondensacja danych, dziel i zwyciężaj, przetwarzanie rozproszone oraz wiele innych. Głównym zadaniem tych metod jest możliwość analizy dużych zbiorów danych w rozsądnym czasie w celu wydobycia interesującej wiedzy. Ważną kwestią jest wstępne przygototwanie danych do dalszej analizy. Część badań  koncentruje się na zmniejszeniu złożoności danych wejściowych, ponieważ nawet najbardziej zaawansowana technologia komputerowa w większości przypadków nie jest w stanie wydajnie przetworzyć całych danych wejściowych przy użyciu jednej maszyny. Wykorzystanie wiedzy domenowej do zaprojektowania operatora przetwarzania wstępnego jest jednym z rozwiązań dla dużych zbiorów danych. Często też wykorzystuje się systemy chmurowe w celu wstępnego przetworzenia surowych danych.


\begin{table}[h!]
    \centering
  \begin{tabular}{ |p{4cm}||p{4cm}|p{4cm}|  }
 \hline
Cecha& Small Data& Big Data\\
 \hline
 \hline
Objętość & Ograniczona-duża & Bardzo duża\\
\hline
Szybkość & Powolna, zamrożone ramki/pakiety & Szybka, ciagła\\
\hline
Różnorodność & Ograniczona & Szeroki zakres\\
\hline
Ograniczenia & Próbki & Cała populacja\\
\hline
Rozdzielczość i indeksykowalność & Ciągła i słaba, surowa i silna & surowa i silna\\
\hline
Relacyjność & Słaba-silna & Silna\\
\hline
Rozszerzalność i skalowalność & Mała-średnia & Duża\\
 \hline
\end{tabular}
    \caption{Porównanie Small Data oraz Big Data}
    \label{tab:my_label}
\end{table}



\subsection{Metody próbkowania}
\label{sampling_methods}
\subsubsection{Oversampling.} 
\hfill \break
Metody oversamplingu polegają na zrównoważeniu rozkładu prawdopodobieństwa apriori pomiędzy klasami. Dzięki wiedzy na temat klas problemu możemy przy pomocy określonego algorytmu stworzyć dane syntetyczne dla klasy mniejszościowej, które rozkład apriori pomiędzy klasami. Poniżej znajdują się najpopularniejsze metody:
\begin{itemize}

    \item\textbf{Borderline-SMOTE} - algorytm ten wychodzi z założenia, że próbki znajdujące się daleko od granicy mogą w niewielkim stopniu zwiększyć powodzenie klasyfikacji. Technika ta identyfikuje próbki znajdujące się w pobliżu granicy.
    \newline
    \item \textbf{AHC} - Ta metoda używa klasteryzacji do generowania danych syntetycznych do zrównoważenia rozkładu danych między klasami. Do tego celu został użyty algorytm centroidów.
    \newline
    \item \textbf{ADASYN} - Główna idea tego algorytmu wywodzi się z wykorzystania rozkładu ważonego w zależności od rodzaju przykładów mniejszościowych zgodnie z ich zdolnością do uczenia się. Ilość danych syntetycznych dla każdego z nich jest związana z poziomem trudności każdego przykładu mniejszościowego.
    \newline
    \item \textbf{DBSMOTE} - Ten algorytm opiera się klastrowaniu w oparciu o gęstość. Dane syntetyczne generowane są po  najkrótszej ścieżce od każdej mniejszościowej instancji do pseudocentroidu klastra klasy mniejszościowej.
    \newline
\end{itemize}
\subsubsection{Undersampling.}
\hfill \break
Metody Undersamplingu mają za zadanie zmniejszyć ilość danych klasy mniejszościowej bez utraty istotnych informacji. Takie działanie jest korzystne w przypadku kiedy mamy do czynienia z dużymi zbiorami danych i ich przetwarzanie jest bardzo kosztowne obliczeniowo. Poniżej przedstawiamy najpopularniejsze metody:

\begin{itemize}
    \item \textbf{Random under-sampling(RUS)} - w tej metodzie nieheurystycznej równoważenie zbiorów danych odbywa się poprzez losowe usuwanie niektórych próbek klas większościowych. Random under-smapling polega na losowym wybieraniu przykładów z klasy większości i usuwaniu ich ze zbioru danych uczących. W tej metodzie instancje klas większości są losowo odrzucane, aż do osiągnięcia bardziej zrównoważonego rozkładu. Obok random over-samplingu jest to druga metoda "naiwnego próbkowania ponownego", ponieważ nie zakłada niczego na temat danych oraz nie są używane żadne heurystyki. Z tego powodu metoda ta jest prosta do wdrożenia oraz szybka do wykonania co jest ważną cechą w przypadku dużych i złożonych zbiorów danych. Metoda ta może być zastosowana do klasyfiakcji binarnej oraz problemów klasyfikacji wieloklasowej z jedną lub większą liczbą klas większościowych lub mniejszościowych. Zmiana rozkładu klas ma wpływ jedynie na zestaw danych uczących. Nie stosuje się go do testowego zestawu danych używanego do oceny wydajności modelu.
    \newline
    \item \textbf{Condensed nearest neighbor rule(CNN)} - metoda polega na eliminacji próbek klasy większościowej, które są odległe od granicy decyzji, ponieważ te próbki możemy uznać za mniej znaczące w procesie nauki. Najpierw losowo wybierana jest próba z klasy większościowej i utworzony podzbiór z wszystkimi próbkami klas mniejszościowych. Następnie 1-NN jest używany w tym podzbiorze, aby sklasyfikować inne próbki z klasy większościowej. Każda błędnie sklasyfikowana próbka z klasy większościowej jest brana do ponownego utworzenia zestawu danych próbkowanych. 
    \break
    \newline
    \textbf{Algorytm CNN:}
    \begin{enumerate}
        \item  Pierwsza próbka jest umieszczana w zbiorze STORE.
        
        \item  Druga próbka jest klasyfikowana zgodnie z regułą NN, stosując jako odniesienie zawartość STORE. Jeśli druga próbka zostanie sklasyfikowana poprawnie wówczas jest umieszczana w zbiorze GRABBAG, jeśli nie to jest umieszczana w STORE.
        
        \item  Postępowanie indukcyjne, i-ta próbka jest klasyfikowana przez obecną zawartość STORE. Jeżeli sklasyfikuje poprawnie, to zostanie umieszczona w zbiorze GRABAG, w przeciwnym przypadku trafi do STORE.
        
        \item Po jednym przejściu przez oryginalne próbki, procedura jest powtarzana w pętli aż do momentu kiedy zbiór GRABBAG zostanie wyczerpany, lub  jeśli jedna iteracja przez zbiór GRABBAG nie powoduje przeniesienia próbek do zbioru STORE.
    
        \item Finalna zawartość STORE służy jako punkty odniesienia dla reguły najbliższego sąsiada. Natomiast zawartość GRABBAG jest odrzucana.
        
    \end{enumerate}
    
    \newline
    \item \textbf{Tomek links(TL)} - metoda jest przeciwieństwem metody CNN. Próbki graniczne mogą być traktowane jako niebezpieczne, ponieważ niewielka zmiana może spowodować przypisanie ich do niewłaściwej klasy.Każda próbka służy do znalezienia innej próbki, która ma minimalną odległość między nimi. Jeżeli te dwie próbki znajdą się w różnych klasach, próbka z klasy większościowej zostanie usunięta. Metoda ta może spowodować wzrost obszaru decyzyjnego. Metoda ta jest rozszerzeniem metody Nearest-Neighbour Rule (NNR). 
    \newline
    \textbf{Działanie algorytmu:}
    \begin{enumerate}
        \item Niech x będzie instancją klasy A a y instancją klasy B.
        \item Niech d(x,y) będzie odległością między x i y.
        \item(x,y) to T-link, jeśli w każdym przypadku z, d(x,y)<d(x,z) lub d(x,y)<d(y,z)
        \item Jeśli jakiekolwiek dwa przykłady to T-link, to jeden z nich to szum inaczej oba przykłady znajdują się na granicy klas.
    \end{enumerate}

    Metoda T-link może być stosowana jako metoda kierowanego undersamplingu.
    \break
    \item \textbf{One-sided selection(OSS)} - metoda stosuje metody Tomek links, a następnie Condensed nearest neighbor rule. Dzięki zastosowaniu tych dwóch technik, pozostałe próbki klasy większościowej są bardziej przydatne do nauki.
    
    \begin{figure}[h!]
\includegraphics[width=\textwidth]{obrazek.png}
\caption{Rozkład danych niezbalansowanych} 
\label{fig:obrazek}
\end{figure}
    
    \break
    Rysunek nr 3 pokazuję, iż negatywne próbki mogą być podzielone na 4 części.
    1. Próbki, które cierpią z powodu szumu etykiety klasy, na przykład próbka znajdująca się w lewym dolnym rogu.
    \newline

    2. Próbki będące na pograniczu mogą być niewiarygodne ponieważ niewielki szum może spowodować ich błędne sklasyfikowanie.
    \newline
    
    3. Próbki, które są zbędne a ich część może być reprezentowana przez inne punkty. Takim przykładem są punkty w prawym górnym rogu.
    \newline
  
    4.Bezpieczne przykłądy które warto zachować do dalszych etapów klasyfikacji.
    \newline

    Zbędne przykłady nie wpływają negatywnie na poroces klasyfikacji, lecz zwiększają jej koszt. Rysunek nr 5 pokazuje usuwanie zbędnych próbek negatywnych.
    \newline

    
    \begin{figure}[h!]
\includegraphics[width=\textwidth]{obrazek_3.png}
\caption{Po usunięciu zbędnych danych negatywnych} 
\label{fig:obrazek_2}
\end{figure}
    \break
    Korzystając z koncepcji Tomek links, bierzemy dwie  próbki x i y, o różnych etykietach oraz oznaczamy dystans pomiędzy nimi.Para x i y jest oznaczana jako  "Tomek links" jeżeli nie ma takich przykładów że, \newline
    δ(x,z)<δ(x,y) lub δ(y,z)<δ(y,x)\newline
    Próbę zmniejszenia liczby zbędnych przykładów można potraktować jako zadanie stworzenia spójnego podzbioru C zbioru uczącego S. Z definicji zbiór C zawierający się w S jest zgodny z S, jeśli użyty przez regułę 1-NN poprawnie sklasyfikuje przykłady w S. Należy zauważyć, iż każdy zbiór jest spójny sam w sobie. Nie zależy nam jednak na stworzeniu najmniejszego zbioru C. Wystarczy jedynie, iż zbiór wartości negatywnych wystarczająco się skurczy. W tym celu można użyć np. techniki Hart. Zaczynamy z jedną negatywną próbką oraz wszystkimi pozytywnymi próbkami umieszczonymi w C. Następnie za pomocą reguły 1-NN z przykładami w zbiorze C w celu ponowej reklasyfikacji zbioru S. Wtedy próbki, które zostały wcześniej błędnie pominięte zostaną dodane.\newline
    

    \item \textbf{Neihborhood Cleaning Rule(NCL)}- wykorzystuję edytowaną regułę najbliższego sąsiada Wilsona (ENN), aby usunąć niektóre próbki klasy większościowej. Początkowo, odnajdywanych jest trzech najbliższych sąsiadów.Jeżeli wybrana próbka należy do klasy większościowej, ale algorytm trzech najbliższych sąsiadó błednie je zakwalifikował, taka próbka zostanie usunięta. Jeżeli wybrana próbka należy do klasy mniejszościowej, ale trzej wybrani sąsiedzi do klasy większościowej, najbliżsi sąsiedzi zostaną usunięci. 
\end{itemize}

\section{Wybrane algorytmy.}
Po dogłębnym przeanalizowaniu tematu uznaliśmy, że do dużych zbiorów danych pasuje idealnie metoda udersamplingu. Dzięki niej zmniejszymy ilość danych,  i tak jest ich bardzo dużo. Redukcja danych może odbyć się bez większych strat informacyjnych ponieważ dane często się duplikują.  Metody, które wybraliśmy to:
\begin{itemize}
    \item Random Undersampling
    \item One side selection
\end{itemize}
Wybraliśmy dwie metody, które są zróżnicowane. RUS wybraliśmy z powodu możliwości modyfikacji w celu lepszego wybierania danych do usunięcia. Metodę one side wybraliśmy że wzgledu na to, że jest bardziej zaawansowana i powinna teoretycznie lepiej wybierać dane, które można usunąć. Sprawdzimy jak te dwa algotrytmy radzą sobie na różnych zbiorach dużych danych. Dane będą niezbilansowane, lecz stopień ich niezbilansowania będzię różny. Po wykonaniu wstępnych badań, spróbujemy dokonać modyfikacji algorytmów i sprawdzimy czy modyfikacje pozwoliły na polepszenie jakości metryk.

\section{Hipoteza.}
Zmniejszenie liczby danych w klasie większościowej w dużych zbiorach danych nie powoduje znacznego pogorszenia predykcji w procesie inferencji. Metoda pozwalająca w jednoznaczny sposób określić dane informatywne może  w znaczący sposób zwiększyć skuteczność predykcji.

\bibliographystyle{plain}
\bibliography{xd.bib, Fernandez.bib, OSS.bib, tlink.bib, CNN.bib, Bigdata.bib, Bigdata2.bib, undersampling.bib, undersampling2.bib, undersampling3.bib, usampling4.bib, 10.bib, uny.bib, cnn.bib}
\phantom{\cite{article}
\cite{10.5555/3241691.3241712}
\cite{Tsai2015BigDA}
\cite{doi:10.1177/2053951716631130}
\cite{8921159}
\cite{inproceedings}
\cite{elhassan2016classification}
\cite{inproceedingss}
\cite{8982391}
\cite{4717268}
\cite{6891771}
\cite{8424689}
\cite{10.5555/2559492}
\cite{hart1968condensed}
}

\end{document}
